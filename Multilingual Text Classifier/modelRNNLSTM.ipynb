{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4QBY5rT7OAs",
        "outputId": "fd95f8f7-fe0c-41b6-ca94-b93d0521e328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIhkUXdJ7Yhq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgMyUvqc7a5x",
        "outputId": "8d19fef5-3643-4956-83ca-f4de24b1d523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mvrxqqb7yne"
      },
      "outputs": [],
      "source": [
        "training_set = pd.read_csv('balanced_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbjd2khb7e8L"
      },
      "outputs": [],
      "source": [
        "def filtered_cmnt(comment):\n",
        "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = word_tokenize(comment)\n",
        "    filtered_words = [word for word in words if word.lower() not in combined_stopwords and word.isalpha()]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply preprocessing to the comments\n",
        "training_set['filtered-comment'] = training_set['comment'].apply(filtered_cmnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wjgvJV_a-O0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R79AzYipcdSz"
      },
      "outputs": [],
      "source": [
        "# pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83qL3xKzcCDk"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train the SentencePiece model using your file 'data.txt'\n",
        "spm.SentencePieceTrainer.Train('--input=data.txt --model_prefix=m --vocab_size=32000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd3F9DKOdyNT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FkBL1AN74iH",
        "outputId": "4fe49448-071f-4621-ae14-89df40a43d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comment  \\\n",
            "0                                        jai mata de   \n",
            "1  that was a very helpful lecture your teaching ...   \n",
            "2       sir one quistion please of motion in a plane   \n",
            "3  euglena and chlamydomonas like organisms give ...   \n",
            "4                         last wala answer galat hai   \n",
            "\n",
            "                                   tokenized_comment  \n",
            "0                                 [▁jai, ▁mata, ▁de]  \n",
            "1  [▁that, ▁was, ▁a, ▁very, ▁helpful, ▁lecture, ▁...  \n",
            "2  [▁sir, ▁one, ▁qui, stion, ▁please, ▁of, ▁motio...  \n",
            "3  [▁euglen, a, ▁and, ▁chlamydomonas, ▁like, ▁org...  \n",
            "4              [▁last, ▁wala, ▁answer, ▁galat, ▁hai]  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the trained SentencePiece model\n",
        "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('balanced_train.csv')  # Replace with your actual file path\n",
        "\n",
        "# Assume your CSV has a 'comment' column that contains sentences\n",
        "comments = df['comment']  # Extract the 'comment' column\n",
        "\n",
        "# Define a function to tokenize each sentence using SentencePiece\n",
        "def tokenize_sentence(sentence):\n",
        "    return sp.encode_as_pieces(sentence)  # Tokenize each sentence into subwords\n",
        "\n",
        "# Apply the tokenization function to each sentence in the 'comment' column\n",
        "df['tokenized_comment'] = comments.apply(tokenize_sentence)\n",
        "\n",
        "# Now, df['tokenized_comment'] contains the tokenized sentences\n",
        "print(df[['comment', 'tokenized_comment']].head())  # Display a sample of the original and tokenized comments\n",
        "\n",
        "# Optionally, save the new DataFrame to a CSV file\n",
        "df.to_csv('tokenized_comments.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq_EecoUerRs"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DSD4jKdejrE"
      },
      "outputs": [],
      "source": [
        "# Assuming df['tokenized_comment'] contains tokenized comments as lists of tokens\n",
        "def tokenize_and_pad(comments, sp, max_len=30):\n",
        "    # Convert tokenized comments into token IDs\n",
        "    token_ids = [sp.encode_as_ids(comment) for comment in comments]\n",
        "\n",
        "    # Pad the sequences to ensure uniform length\n",
        "    padded_ids = pad_sequences(token_ids, maxlen=max_len, padding='post')\n",
        "\n",
        "    return np.array(padded_ids)\n",
        "\n",
        "# Convert and pad the tokenized comments\n",
        "X = tokenize_and_pad(df['comment'], sp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Vo9nVUOUfz"
      },
      "outputs": [],
      "source": [
        "# Define a custom mapping for labels\n",
        "label_mapping = {\n",
        "    'doubt': 0,\n",
        "    'feedback': 1,\n",
        "    'irrelevant': -1\n",
        "}\n",
        "\n",
        "# Map the labels to their numerical values\n",
        "df['label'] = df['label'].map(label_mapping)\n",
        "\n",
        "# Check for any labels that couldn't be mapped\n",
        "if df['label'].isnull().any():\n",
        "    print(\"Warning: Some labels could not be mapped. Please check your dataset.\")\n",
        "    print(df[df['label'].isnull()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqG9SpoogWmB"
      },
      "outputs": [],
      "source": [
        "y = df['label'].values  # Extract the mapped labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8swsKJ1LAiYC"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXO3kJyNg4K-",
        "outputId": "7b3a13a7-0e18-477c-b2fd-e41c41e0710e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165726, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZZAtJGXg4DA",
        "outputId": "cc97ddf8-5779-41e3-d7ff-2799633f3030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (132580, 30)\n",
            "Shape of y_train: (132580,)\n",
            "Sample from X_train: [   44  1076    65 17352  2100   136  1207   756   363    24     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0]\n",
            "Corresponding label in y_train: -1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check the shape of X_train and y_train\n",
        "print(\"Shape of X_train:\", X_train.shape)  # Should be (num_train_samples, 50)\n",
        "print(\"Shape of y_train:\", y_train.shape)   # Should be (num_train_samples,)\n",
        "\n",
        "# Display sample data from X_train and y_train\n",
        "print(\"Sample from X_train:\", X_train[0])   # Display the first training sample\n",
        "print(\"Corresponding label in y_train:\", y_train[0])  # Display the corresponding label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpzB-doNiCWR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M63GH-AmigZ0"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(sp)  # Size of the vocabulary from SentencePiece\n",
        "embedding_dim = 128    # Choose an appropriate embedding dimension (e.g., 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ygyPVNjU0_"
      },
      "outputs": [],
      "source": [
        "# Adjust the labels for compatibility with sparse_categorical_crossentropy\n",
        "y_train_adjusted = np.where(y_train == -1, 2, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NOvw68d7go0"
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "# Add the embedding layer with the input length specified\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=50))\n",
        "model.add(LSTM(128, return_sequences=False))  # LSTM layer\n",
        "model.add(Dropout(0.5))                   # Dropout layer for regularization\n",
        "# Flatten the output to feed into ANN\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))    # Output layer for 3 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rguATIFchyuT",
        "outputId": "c4aeca0d-774b-4491-8d69-de296c141bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 133ms/step - accuracy: 0.5842 - loss: 0.8605 - val_accuracy: 0.6953 - val_loss: 0.7006\n",
            "Epoch 2/4\n",
            "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 138ms/step - accuracy: 0.7271 - loss: 0.6459 - val_accuracy: 0.6919 - val_loss: 0.7126\n",
            "Epoch 3/4\n",
            "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 132ms/step - accuracy: 0.7588 - loss: 0.5774 - val_accuracy: 0.6920 - val_loss: 0.7121\n",
            "Epoch 4/4\n",
            "\u001b[1m1658/1658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 130ms/step - accuracy: 0.7777 - loss: 0.5207 - val_accuracy: 0.6879 - val_loss: 0.7684\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train_adjusted, epochs=4, batch_size=64, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtKw_axRtG9x",
        "outputId": "e64e2253-42fc-4739-e954-3e4064ff866a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2863/2863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 34ms/step\n",
            "   id       label\n",
            "0   0  irrelevant\n",
            "1   1  irrelevant\n",
            "2   2    feedback\n",
            "3   3    feedback\n",
            "4   4       doubt\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define your custom mapping for labels\n",
        "label_mapping = {\n",
        "    0: 'doubt',\n",
        "    1: 'feedback',\n",
        "    2: 'irrelevant'  # Adjusted label for -1 during training\n",
        "}\n",
        "\n",
        "# Load your test CSV file\n",
        "test_df = pd.read_csv('test.csv')  # Replace with your actual test file path\n",
        "\n",
        "# Tokenize the comments in the test set\n",
        "test_comments = test_df['comment']\n",
        "test_df['tokenized_comment'] = test_comments.apply(lambda x: sp.encode_as_ids(x))\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "max_length = 50  # Define the maximum length (use the same as your training set)\n",
        "X_test = pad_sequences(test_df['tokenized_comment'].tolist(), maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions using your trained model\n",
        "predicted_labels = model.predict(X_test)\n",
        "\n",
        "# Get the predicted class indices (0, 1, or 2)\n",
        "predicted_indices = np.argmax(predicted_labels, axis=1)\n",
        "\n",
        "# Convert numerical predictions back to string labels\n",
        "predicted_strings = [label_mapping[label] for label in predicted_indices]\n",
        "\n",
        "# Prepare the DataFrame to save predictions\n",
        "output_df = pd.DataFrame({\n",
        "    'id': test_df['id'],  # Assuming 'id' is a column in your test set\n",
        "    'label': predicted_strings\n",
        "})\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output_df.to_csv('thakgayahoon02.csv', index=False)\n",
        "\n",
        "# Optionally, display the predictions\n",
        "print(output_df.head())  # Show the first few predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c2WU34Waww-Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}